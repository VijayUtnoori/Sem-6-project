{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e20c902-4adf-4e42-884b-52929fcefd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"tweet_eval_train.csv\")\n",
    "val_df   = pd.read_csv(\"tweet_eval_validation.csv\")\n",
    "test_df  = pd.read_csv(\"tweet_eval_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e2d6c9-c96b-4f90-a46e-1d3bcebebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_clean(df):\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    df[\"clean_text\"] = df[\"text\"].str.strip().str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e02d6e-b356-488e-b054-69fc381911b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = minimal_clean(train_df)\n",
    "val_df   = minimal_clean(val_df)\n",
    "test_df  = minimal_clean(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacded1a-37f1-4194-9a68-c8745a4411a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  \"QT @user In the original draft of the 7th boo...   \n",
      "1  \"Ben Smith / Smith (concussion) remains out of...   \n",
      "2  Sorry bout the stream last night I crashed out...   \n",
      "3  Chase Headley's RBI double in the 8th inning o...   \n",
      "4  @user Alciato: Bee will invest 150 million in ...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  \"qt @user in the original draft of the 7th boo...  \n",
      "1  \"ben smith / smith (concussion) remains out of...  \n",
      "2  sorry bout the stream last night i crashed out...  \n",
      "3  chase headley's rbi double in the 8th inning o...  \n",
      "4  @user alciato: bee will invest 150 million in ...  \n",
      "                                                text  \\\n",
      "0  Dark Souls 3 April Launch Date Confirmed With ...   \n",
      "1  \"National hot dog day, national tequila day, t...   \n",
      "2  When girls become bandwagon fans of the Packer...   \n",
      "3  @user I may or may not have searched it up on ...   \n",
      "4  Here's your starting TUESDAY MORNING Line up a...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  dark souls 3 april launch date confirmed with ...  \n",
      "1  \"national hot dog day, national tequila day, t...  \n",
      "2  when girls become bandwagon fans of the packer...  \n",
      "3  @user i may or may not have searched it up on ...  \n",
      "4  here's your starting tuesday morning line up a...  \n",
      "                                                text  \\\n",
      "0  @user @user what do these '1/2 naked pics' hav...   \n",
      "1  OH: “I had a blue penis while I was this” [pla...   \n",
      "2  @user @user That's coming, but I think the vic...   \n",
      "3  I think I may be finally in with the in crowd ...   \n",
      "4  @user Wow,first Hugo Chavez and now Fidel Cast...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  @user @user what do these '1/2 naked pics' hav...  \n",
      "1  oh: “i had a blue penis while i was this” [pla...  \n",
      "2  @user @user that's coming, but i think the vic...  \n",
      "3  i think i may be finally in with the in crowd ...  \n",
      "4  @user wow,first hugo chavez and now fidel cast...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df[[\"text\", \"clean_text\"]].head())\n",
    "print(val_df[[\"text\", \"clean_text\"]].head())\n",
    "print(test_df[[\"text\", \"clean_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6434abf-4eba-4d5c-9f8e-789065426029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'clean_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25c95ef-24d7-43c3-b143-4920a2401a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    45.320618\n",
       "2    39.129672\n",
       "0    15.549710\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cb30d4-2628-47c4-b03f-8f6eac663a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    43.45\n",
       "2    40.95\n",
       "0    15.60\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db88db61-1ee2-47ed-8b3b-f2fc55a2dde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    48.331162\n",
       "0    32.334744\n",
       "2    19.334093\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1dc09bd-ead8-4416-9d0e-df249657115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca85daa-8377-486e-ae1c-a17cd7b3e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"qt @user in the original draft of the 7th book, remus lupin survived the battle of hogwarts. #happybirthdayremuslupin\"\n",
      "Tokens: ['\"', 'q', '##t', '@', 'user', 'in', 'the', 'original', 'draft', 'of', 'the', '7th', 'book', ',', 're', '##mus', 'lu', '##pin', 'survived', 'the', 'battle', 'of', 'hog', '##wart', '##s', '.', '#', 'happy', '##bir', '##th', '##day', '##rem', '##us', '##lu', '##pin', '\"']\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_df[\"clean_text\"].iloc[0]\n",
    "print(\"Text:\",sample_text)\n",
    "print(\"Tokens:\",tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2d328d-fa99-49ed-ac49-4aeaaeb2db0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1000,  1053,  2102,  1030,  5310,  1999,  1996,  2434,  4433,\n",
       "          1997,  1996,  5504,  2338,  1010,  2128,  7606, 11320,  8091,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = 20,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddc2440-5672-45f3-8991-0cde7c34be73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[\"input_ids\"]\n",
    "encoded[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "744f86a5-10d4-4e3a-bd9f-acf6047b602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=128\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "test_encodings= tokenizer(\n",
    "    test_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0771ffb-4cb3-4e56-b180-3a3865f600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[\"label\"].tolist()\n",
    "val_labels = val_df[\"label\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019a374c-0a2d-4f99-a8b7-92edb235e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a98506-6126-42f8-b69a-5ffe071eb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a58f38a-e9c8-436d-b512-41d371a49bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5549caa9-996f-46be-99a0-aa6d62063def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0787, -0.0919,  0.1474]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids = sample[\"input_ids\"].unsqueeze(0),\n",
    "        attention_mask = sample[\"attention_mask\"].unsqueeze(0)\n",
    "    )\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f15190-a09d-4502-980e-6a0152842b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('save_tokenizer\\\\tokenizer_config.json',\n",
       " 'save_tokenizer\\\\special_tokens_map.json',\n",
       " 'save_tokenizer\\\\vocab.txt',\n",
       " 'save_tokenizer\\\\added_tokens.json',\n",
       " 'save_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"save_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "141ae40c-48ab-444b-b492-948686664d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_encodings, f)\n",
    "\n",
    "with open(\"val_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_encodings, f)\n",
    "\n",
    "with open(\"test_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_encodings, f)\n",
    "\n",
    "with open(\"labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_labels, val_labels, test_labels), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "872c6010-2749-4f07-966c-7d474c6950c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"usage device:\",device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e8f501-2232-4138-b0e6-efa271320ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e705c152-ec27-405e-b31d-9e54ae63aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56e23142-b010-4dcc-8af8-805e7dd6cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "143898d0-8374-4d3a-8340-8fef51060a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"save_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe4f2505-a34a-4926-8684-bcb5eac62bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_encodings.pkl\", \"rb\") as f:\n",
    "    train_encodings = pickle.load(f)\n",
    "\n",
    "with open(\"val_encodings.pkl\", \"rb\") as f:\n",
    "    val_encodings = pickle.load(f)\n",
    "\n",
    "with open(\"test_encodings.pkl\", \"rb\") as f:\n",
    "    test_encodings = pickle.load(f)\n",
    "\n",
    "with open(\"labels.pkl\", \"rb\") as f:\n",
    "    train_labels, val_labels, test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8510d54a-464c-4b33-8354-43acda166207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=16\n",
    "train_loader=DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader=DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38b277cf-a429-4416-89b9-45de18898d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.14366277, 0.73550041, 0.85186845])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "766e7a46-d938-43c9-b3df-e772031e5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0): np.int64(7093), np.int64(1): np.int64(20673), np.int64(2): np.int64(17849)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32208284-5017-472f-b49e-0c705499ea4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1437, 0.7355, 0.8519], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a8d6475-79e0-4477-894d-1fb06927da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c2eab20-0271-44ab-ac9d-33e804faf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer=AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df00d0bc-c2c9-42ae-9aa8-7664baa087ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0878, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "input_ids=batch[\"input_ids\"].to(device)\n",
    "attention_mask=batch[\"attention_mask\"].to(device)\n",
    "labels=batch[\"labels\"].to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs=model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    "    )\n",
    "loss=criterion(outputs.logits,labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55684a0-7c5b-4547-b2f1-8a2e8c5d1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f8963d2-1ebd-4829-8d0a-2af63c45f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "Training loss: 0.507747250177575\n",
      "Validation loss: 0.6567128905653954\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(\"Training loss:\", avg_train_loss)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(\"Validation loss:\", avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2714b5a0-1509-4b2d-ac71-1d87050a4590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"best_model_epoch1.pt\")\n",
    "print(\"Best model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a5ec2-a525-4b63-8457-265ed84993fb",
   "metadata": {},
   "source": [
    "### Day-3 Summary\n",
    "- Best validation loss: 0.6217\n",
    "- Model saved as: best_model_epoch1.pt\n",
    "- Training stopped due to overfitting\n",
    "- Epoch 1/1\n",
    "-  Training loss: 0.6862482709535086\n",
    "-  Validation loss: 0.6217153009176254\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CUDA)",
   "language": "python",
   "name": "py310cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
