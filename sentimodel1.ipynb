{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e20c902-4adf-4e42-884b-52929fcefd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"tweet_eval_train.csv\")\n",
    "val_df   = pd.read_csv(\"tweet_eval_validation.csv\")\n",
    "test_df  = pd.read_csv(\"tweet_eval_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e2d6c9-c96b-4f90-a46e-1d3bcebebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_clean(df):\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    df[\"clean_text\"] = df[\"text\"].str.strip().str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e02d6e-b356-488e-b054-69fc381911b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = minimal_clean(train_df)\n",
    "val_df   = minimal_clean(val_df)\n",
    "test_df  = minimal_clean(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dacded1a-37f1-4194-9a68-c8745a4411a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  \"QT @user In the original draft of the 7th boo...   \n",
      "1  \"Ben Smith / Smith (concussion) remains out of...   \n",
      "2  Sorry bout the stream last night I crashed out...   \n",
      "3  Chase Headley's RBI double in the 8th inning o...   \n",
      "4  @user Alciato: Bee will invest 150 million in ...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  \"qt @user in the original draft of the 7th boo...  \n",
      "1  \"ben smith / smith (concussion) remains out of...  \n",
      "2  sorry bout the stream last night i crashed out...  \n",
      "3  chase headley's rbi double in the 8th inning o...  \n",
      "4  @user alciato: bee will invest 150 million in ...  \n",
      "                                                text  \\\n",
      "0  Dark Souls 3 April Launch Date Confirmed With ...   \n",
      "1  \"National hot dog day, national tequila day, t...   \n",
      "2  When girls become bandwagon fans of the Packer...   \n",
      "3  @user I may or may not have searched it up on ...   \n",
      "4  Here's your starting TUESDAY MORNING Line up a...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  dark souls 3 april launch date confirmed with ...  \n",
      "1  \"national hot dog day, national tequila day, t...  \n",
      "2  when girls become bandwagon fans of the packer...  \n",
      "3  @user i may or may not have searched it up on ...  \n",
      "4  here's your starting tuesday morning line up a...  \n",
      "                                                text  \\\n",
      "0  @user @user what do these '1/2 naked pics' hav...   \n",
      "1  OH: “I had a blue penis while I was this” [pla...   \n",
      "2  @user @user That's coming, but I think the vic...   \n",
      "3  I think I may be finally in with the in crowd ...   \n",
      "4  @user Wow,first Hugo Chavez and now Fidel Cast...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  @user @user what do these '1/2 naked pics' hav...  \n",
      "1  oh: “i had a blue penis while i was this” [pla...  \n",
      "2  @user @user that's coming, but i think the vic...  \n",
      "3  i think i may be finally in with the in crowd ...  \n",
      "4  @user wow,first hugo chavez and now fidel cast...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df[[\"text\", \"clean_text\"]].head())\n",
    "print(val_df[[\"text\", \"clean_text\"]].head())\n",
    "print(test_df[[\"text\", \"clean_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6434abf-4eba-4d5c-9f8e-789065426029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'clean_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25c95ef-24d7-43c3-b143-4920a2401a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    45.320618\n",
       "2    39.129672\n",
       "0    15.549710\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20cb30d4-2628-47c4-b03f-8f6eac663a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    43.45\n",
       "2    40.95\n",
       "0    15.60\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db88db61-1ee2-47ed-8b3b-f2fc55a2dde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    48.331162\n",
       "0    32.334744\n",
       "2    19.334093\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1dc09bd-ead8-4416-9d0e-df249657115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca85daa-8377-486e-ae1c-a17cd7b3e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"qt @user in the original draft of the 7th book, remus lupin survived the battle of hogwarts. #happybirthdayremuslupin\"\n",
      "Tokens: ['\"', 'q', '##t', '@', 'user', 'in', 'the', 'original', 'draft', 'of', 'the', '7th', 'book', ',', 're', '##mus', 'lu', '##pin', 'survived', 'the', 'battle', 'of', 'hog', '##wart', '##s', '.', '#', 'happy', '##bir', '##th', '##day', '##rem', '##us', '##lu', '##pin', '\"']\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_df[\"clean_text\"].iloc[0]\n",
    "print(\"Text:\",sample_text)\n",
    "print(\"Tokens:\",tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec2d328d-fa99-49ed-ac49-4aeaaeb2db0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1000,  1053,  2102,  1030,  5310,  1999,  1996,  2434,  4433,\n",
       "          1997,  1996,  5504,  2338,  1010,  2128,  7606, 11320,  8091,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = 20,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ddc2440-5672-45f3-8991-0cde7c34be73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[\"input_ids\"]\n",
    "encoded[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "744f86a5-10d4-4e3a-bd9f-acf6047b602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=128\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "test_encodings= tokenizer(\n",
    "    test_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0771ffb-4cb3-4e56-b180-3a3865f600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[\"label\"].tolist()\n",
    "val_labels = val_df[\"label\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "019a374c-0a2d-4f99-a8b7-92edb235e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99a98506-6126-42f8-b69a-5ffe071eb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a58f38a-e9c8-436d-b512-41d371a49bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5549caa9-996f-46be-99a0-aa6d62063def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0884,  0.0145, -0.0277]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids = sample[\"input_ids\"].unsqueeze(0),\n",
    "        attention_mask = sample[\"attention_mask\"].unsqueeze(0)\n",
    "    )\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83f15190-a09d-4502-980e-6a0152842b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('save_tokenizer\\\\tokenizer_config.json',\n",
       " 'save_tokenizer\\\\special_tokens_map.json',\n",
       " 'save_tokenizer\\\\vocab.txt',\n",
       " 'save_tokenizer\\\\added_tokens.json',\n",
       " 'save_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"save_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "141ae40c-48ab-444b-b492-948686664d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_encodings, f)\n",
    "\n",
    "with open(\"val_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_encodings, f)\n",
    "\n",
    "with open(\"test_encodings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_encodings, f)\n",
    "\n",
    "with open(\"labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_labels, val_labels, test_labels), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ae131-4905-40f3-b7f7-ef4b894c420d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
